---
apiVersion: v1
kind: Namespace
metadata:
  name: ${TEST_NAMESPACE}
  labels:
    name: ${TEST_NAMESPACE}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-job-${JOB_SUFFIX}
  namespace: ${TEST_NAMESPACE}
data:
  example_job.py: |
    #!/usr/bin/env python3
    import os
    import time
    from typing import Optional, Sequence

    from pyspark.sql import SparkSession
    from pyspark.sql.functions import rand


    def run(args: Optional[Sequence[str]] = None) -> None:
        """Entry point used by the shared driver."""
        spark = SparkSession.builder.appName("spot-balancer-sample").getOrCreate()

        # Generate synthetic data and force a few stages with shuffles
        n_rows = int(os.getenv("SB_ROWS", "20000000"))
        n_parts = int(os.getenv("SB_PARTS", "200"))
        sleep_per_task = int(os.getenv("SB_TASK_SLEEP_SEC", "300"))

        df = spark.range(0, n_rows).repartition(n_parts)
        df = df.withColumn("x", rand())
        df = df.orderBy("x")

        # Run tasks across executors and keep tasks alive briefly
        def _linger(_iter):
            time.sleep(sleep_per_task)
            yield 1

        count = df.rdd.mapPartitions(_linger).count()
        print(f"result={count}")

        # Keep the driver up a little longer to allow observation
        driver_sleep = int(os.getenv("SB_DRIVER_SLEEP_SEC", "300"))
        time.sleep(driver_sleep)
        spark.stop()


    if __name__ == "__main__":
        import sys

        run(sys.argv[1:])
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: spark-sa
  namespace: ${TEST_NAMESPACE}
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: spark-role
  namespace: ${TEST_NAMESPACE}
rules:
- apiGroups: [""]
  resources: ["pods", "services", "configmaps"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: [""]
  resources: ["pods/log"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: spark-role-binding
  namespace: ${TEST_NAMESPACE}
subjects:
- kind: ServiceAccount
  name: spark-sa
  namespace: ${TEST_NAMESPACE}
roleRef:
  kind: Role
  name: spark-role
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: v1
kind: Service
metadata:
  name: spark-pi-${JOB_SUFFIX}-headless
  namespace: ${TEST_NAMESPACE}
spec:
  clusterIP: None
  selector:
    spark-app-name: spark-pi-${JOB_SUFFIX}
  ports:
  - name: driver-rpc-port
    protocol: TCP
    port: 7078
    targetPort: 7078
  - name: blockmanager
    protocol: TCP
    port: 7079
    targetPort: 7079
---
apiVersion: v1
kind: Pod
metadata:
  name: spark-pi-${JOB_SUFFIX}
  namespace: ${TEST_NAMESPACE}
  labels:
    spark-role: driver
    spark-app-name: spark-pi-${JOB_SUFFIX}
    emr-containers.amazonaws.com/job.id: spark-pi-${JOB_SUFFIX}
    app: spark-pi
  annotations:
    workload/spot-ratio: "${SPOT_RATIO}"
spec:
  serviceAccountName: spark-sa
  restartPolicy: Never
  hostname: spark-pi-${JOB_SUFFIX}
  subdomain: spark-pi-${JOB_SUFFIX}-headless
  volumes:
  - name: spark-job
    configMap:
      name: spark-job-${JOB_SUFFIX}
      defaultMode: 0755
  containers:
  - name: spark-driver
    image: apache/spark:3.5.0
    volumeMounts:
    - name: spark-job
      mountPath: /opt/spark-job
    command:
    - /opt/spark/bin/spark-submit
    - --master
    - k8s://https://kubernetes.default.svc.cluster.local:443
    - --deploy-mode
    - client
    - --name
    - spark-pi-${JOB_SUFFIX}
    - --conf
    - spark.app.name=spark-pi-${JOB_SUFFIX}
    - --conf
    - spark.kubernetes.namespace=${TEST_NAMESPACE}
    - --conf
    - spark.kubernetes.authenticate.driver.serviceAccountName=spark-sa
    - --conf
    - spark.executor.instances=${EXECUTOR_COUNT}
    - --conf
    - spark.executor.memory=4g
    - --conf
    - spark.executor.cores=2
    - --conf
    - spark.kubernetes.executor.request.cores=2
    - --conf
    - spark.kubernetes.executor.limit.cores=2
    - --conf
    - spark.driver.memory=4g
    - --conf
    - spark.driver.host=spark-pi-${JOB_SUFFIX}.spark-pi-${JOB_SUFFIX}-headless.${TEST_NAMESPACE}.svc.cluster.local
    - --conf
    - spark.driver.port=7078
    - --conf
    - spark.driver.blockManager.port=7079
    - --conf
    - spark.kubernetes.executor.label.emr-containers.amazonaws.com/job.id=spark-pi-${JOB_SUFFIX}
    - --conf
    - spark.kubernetes.executor.label.app=spark-pi
    - --conf
    - spark.kubernetes.node.selector.spark-role=executor
    - --conf
    - spark.kubernetes.container.image=apache/spark:3.5.0
    - --conf
    - spark.kubernetes.driver.pod.name=spark-pi-${JOB_SUFFIX}
    - /opt/spark-job/example_job.py
    - "500"
    resources:
      requests:
        memory: "4Gi"
        cpu: "1000m"
      limits:
        memory: "6Gi"
        cpu: "2000m"
